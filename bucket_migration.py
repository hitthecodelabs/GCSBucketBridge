# -*- coding: utf-8 -*-
"""bucket_migration.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1dNG0IY9fLtiLTpz-ELjwAuDWO59koS-S

#### Login using the gcloud CLI
"""

!gcloud auth login

"""#### Set the project that contains your bucket"""

# !gcloud config set project project-id-1
!gcloud config set project project-id-2

"""#### CREATE A SERVICE ACCOUNT | just in case you dont have one"""

# !gcloud iam service-accounts create readonly-data-viewer --description="Service account for read-only access to Datastore" --display-name="Datastore ReadOnly"

"""#### Set the service account permissions"""

# !gcloud projects add-iam-policy-binding project-id-2 --member=serviceAccount:readonly-data-viewer@project-id-2.iam.gserviceaccount.com --role=roles/storage.objectViewer
# !gcloud projects add-iam-policy-binding project-id-2 --member=serviceAccount:readonly-data-viewer@project-id-2.iam.gserviceaccount.com --role=roles/storage.objectCreator

"""#### Get the keys"""

# !gcloud iam service-accounts keys create ./datastore-readonly3-key.json --iam-account readonly-data-viewer@project-id-2.iam.gserviceaccount.com

"""#### Once you conclude the work, remove risky permissions"""

# !gcloud projects remove-iam-policy-binding steam-project-id-2 --member=serviceAccount:readonly-data-viewer@steam-project-id-2.iam.gserviceaccount.com --role=roles/storage.objectCreator

"""#### Let's begin the game"""

import tempfile
from glob import glob
from google.cloud import storage

"""#### The functions I created for this task

"""

def get_files(bucket_name):
    """
    Lists all files (blobs) in a given Google Cloud Storage bucket.

    Args:
        bucket_name (str): The name of the source Google Cloud Storage bucket.

    Returns:
        list: A list of strings, where each string is the name of a blob (file) in the bucket.
    """
    storage_client = storage.Client() # Initialize the Google Cloud Storage client within the function scope
    blobs = storage_client.list_blobs(bucket_name)  ### Bucket initialization and list blobs operation
    files = [blob.name for blob in blobs]           ### Extract blob names to create a list of file names
    return files

def download_tmp_file(storage_client, bucket_name, blob_source, namef=''):
    """
    Downloads a specific blob (file) from a Google Cloud Storage bucket to a temporary local file.

    Args:
        storage_client (google.cloud.storage.client.Client): Initialized Google Cloud Storage client.
        bucket_name (str): The name of the source Google Cloud Storage bucket.
        blob_source (str): The name of the blob (file path within the bucket) to download.
        namef (str, optional):  An optional filename to use for the temporary local file.
                                If not provided, a unique temporary file name will be generated. Defaults to ''.

    Returns:
        str: The path to the temporary local file where the blob was downloaded.
    """
    bucket = storage_client.bucket(bucket_name) # Initialize bucket object
    blob = bucket.blob(blob_source)             # Initialize blob object for the specified source
    if namef!="":
        temp_local_path=f"/tmp/{namef}" # Construct a specific path in /tmp if namef is provided
    else:
        _, temp_local_path = tempfile.mkstemp()     # Create a unique temporary file path
    print(temp_local_path) # Print the path of the temporary file for debugging or monitoring
    folder = "/".join(temp_local_path.split("/")[:-1]) # Extract the directory path from the temporary file path
    os.makedirs(folder, exist_ok=True) # Ensure the directory for the temporary file exists, creating it if necessary
    blob.download_to_filename(temp_local_path)  # Download the blob to the temporary local file
    return temp_local_path

def upload_file(bucket_name, local_file, dest_buck_file):
    """
    Uploads a local file to a Google Cloud Storage bucket.

    Args:
        bucket_name (str): The name of the destination Google Cloud Storage bucket.
        local_file (str): The path to the local file to upload.
        dest_buck_file (str): The desired destination path (blob name) for the file in the bucket.
    """
    storage_client = storage.Client() # Initialize the Google Cloud Storage client within the function scope
    bucket = storage_client.bucket(bucket_name) ### bucket initialize
    blob = bucket.blob(dest_buck_file)          ### destiny directory in the bucket
    blob.upload_from_filename(local_file)       ### Upload the local file to the specified destination in the bucket

"""#### Set the location of your keys"""

import os

# os.environ["GOOGLE_APPLICATION_CREDENTIALS"] = "/content/datastore-readonly2-key.json" ### keys from account 1
os.environ["GOOGLE_APPLICATION_CREDENTIALS"] = "/content/datastore-readonly3-key.json"   ### keys from account 2

"""#### Initialize your bucket"""

storage_client = storage.Client()

# bucket_name = 'bucket_name_1'
bucket_name = 'bucket_name_2'
blobs = storage_client.list_blobs(bucket_name)

"""#### Get the old bucket files into a list, remove those files you don't want to migrate"""

L = get_files(bucket_name=bucket_name)
L

"""#### Download the files into the temp folder in the Google Colab session"""

for filee in L:
    download_tmp_file(storage_client=storage_client,
                      bucket_name=bucket_name,
                      blob_source=filee,
                      namef=f'bucket/{filee}'
                      )

"""#### Finally, upload those files into the new bucket. Remember to set the required permissions to the service account."""

for ff in glob("/tmp/bucket/**/*.*", recursive=True):
    # print(ff)
    dest = ff.replace("/tmp/bucket/", "")
    print(ff, dest)
    upload_file(bucket_name=bucket_name,
                local_file=ff,
                dest_buck_file=dest)

"""#### And that's it. Now you're a professional Google Cloud Storage migrator!"""

